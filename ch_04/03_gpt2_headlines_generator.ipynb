{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b9f9be-bff1-47dc-8be0-576ef6a557ff",
   "metadata": {},
   "source": [
    "# Generating News Headlines using GPT2\n",
    "## GPT2\n",
    "GPT-2 model was released as part of the work titled ‚ÄúLanguage Models are Unsupervised Multi-task Learners‚Äù in 2019. The largest GPT-2 variant is a huge 1.5B parameter transformer-based model which the model was able to perform remarkably well of various NLP tasks. The most striking aspect of this work is that the authors showcase how a model trained in an unsupervised fashion (language modeling) achieves state-of-the-art performance in zero-shot setting.\n",
    "\n",
    "## HuggingFace Transformers\n",
    "One of the most propular python packages to work with Transformer based NLP models. Huggingface transformers is a high-level API to easily load, fine-tune and re-train models such as GPT2, BERT, T5 and so on\n",
    "\n",
    "## Fake Headlines\n",
    "ABC-News Dataset is a dataset of a million headlines available here collected over a period of 17 years. We will make use of this dataset to fine-tune the GPT2 model. Once fine-tuned we will use it to generate some fake headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f3d782-174b-4c55-8d11-8b9a52c34fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install scikit-learn==1.5.1\n",
    "# !pip3 install transformers==4.42.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a457e201-f112-4bbe-9ebb-04a39961ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffef4bc-e884-4509-9636-eedb039e8961",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791b7720-872d-4da9-84a2-d7000e7946fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL\n",
    "# !unzip abcnews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c18110-cd6a-4695-97fe-17f279ec87e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1244184, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv('abcnews-date-text.csv')\n",
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce909286-1c48-45bf-8227-51dccaabca6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05188d98-d126-46f7-aa2b-f9d33483575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833603, 410581)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test= train_test_split(news.headline_text.tolist(),test_size=0.33, random_state=42)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c606838c-86f1-4ca1-ab82-41dd14915a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_dataset.txt','w') as f:\n",
    "  for line in X_train:\n",
    "    f.write(line)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "with open('test_dataset.txt','w') as f:\n",
    "  for line in X_test:\n",
    "    f.write(line)\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e15f55-5921-4489-9a9d-2544626b6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",pad_token='<pad>')\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42589349-ece5-4b79-9159-a81112e500df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=4)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=4)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffdfb77-d42a-4ba1-a131-8687ec77beb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghavbali/.pyenv/versions/3.11.9/envs/deeplearning/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035383de-b4fb-470e-8448-d53c62c60911",
   "metadata": {},
   "source": [
    "## Prepare Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d61175-df48-426c-8180-b4f445a88587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Accelerator Device=cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "# MPS/Apple Silicon does not work as intended for this pipeline    \n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "#     Tensor = torch.FloatTensor\n",
    "#     LongTensor = torch.LongTensor\n",
    "#     DEVICE_ID = 0\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = -1\n",
    "print(f\"Backend Accelerator Device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75f6b17-f49a-4284-822c-19903227120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff3d217-f33f-41f5-93fb-b462b88036d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"gpt2-finetuned-headliner\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=512, # batch size for training\n",
    "    per_device_eval_batch_size=256,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    push_to_hub=True,\n",
    "    use_cpu=True # comment this if you have GPU available\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148ff44c-f82e-41de-a595-23aafef9ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b22d1aa-0820-4096-8a3d-d80244d4a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823eb84-895c-4745-bac0-054772cedab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c382996-2764-45c4-bc80-0148a9162e02",
   "metadata": {},
   "source": [
    "## Let us Generate Some Headlines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73670c5-a1f6-40e6-998c-56165b26654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fine-tuned model\n",
    "ft_gpt2_headliner = AutoModelForCausalLM.from_pretrained(\"./headliner\")\n",
    "\n",
    "# setup the generation pipeline\n",
    "headliner = pipeline('text-generation',\n",
    "                     model=ft_gpt2_headliner, \n",
    "                     tokenizer='gpt2',\n",
    "                     pad_token_id=0,\n",
    "                     eos_token_id=50256,\n",
    "                     config={\n",
    "                         'max_length':8,\n",
    "                     },\n",
    "                     device=DEVICE_ID\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2f94852-1ffa-4c03-a9a7-41435c6b02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headline(headliner_pipeline, seed_text=\"News\"):\n",
    "  return headliner_pipeline(seed_text)[0]['generated_text'].split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5912e2af-6576-49fd-9ad9-63b43da657cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'City Council of Sydney announces newcastle cup plans'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headline(headliner, seed_text=\"City Council of Sydney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bbd99-9803-4524-be91-df8a91f9c316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
