{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d821cda-1586-4727-94c1-460f7f145c1e",
   "metadata": {
    "id": "9d821cda-1586-4727-94c1-460f7f145c1e"
   },
   "source": [
    "# Pretraining Optimizations\n",
    "The pretraining step involves the largest amount of data along and is impacted by architectural aspects of the model: its size (parameters), shape (width and depth), and so on.\n",
    "This notebook covers optimization techniques focussed on the pretraining step.\n",
    "\n",
    "We will cover:\n",
    "- Different Floating Point Representations/Formats\n",
    "- Quantization of Floats\n",
    "- Post Training Quantization of Models:\n",
    " - Torch based dynamic quantization\n",
    " - Huggingface and bitsandbytes based 8bit and 4bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec8627-b1c5-441e-8333-c354aa2a2469",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ❗ <b>This Notebook requires GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "njB6s73CpEZZ",
   "metadata": {
    "id": "njB6s73CpEZZ"
   },
   "outputs": [],
   "source": [
    "# !pip3 install -U bitsandbytes\n",
    "# restart after this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978b59cd-0255-4c5d-87eb-4c91047c9f46",
   "metadata": {
    "id": "978b59cd-0255-4c5d-87eb-4c91047c9f46"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import struct\n",
    "import numpy as np\n",
    "from time import time\n",
    "from utils import get_model_size\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, QuantoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "_-0pKdMAv0W1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-0pKdMAv0W1",
    "outputId": "d2a5ff7f-f22c-48a9-9287-02db45320b10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b220cf2c130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f50e6-d91d-431b-b3ed-d528d225cfab",
   "metadata": {
    "id": "614f50e6-d91d-431b-b3ed-d528d225cfab"
   },
   "source": [
    "## Representing Floating Point Numbers\n",
    "\n",
    "<img src=\"./assets/ch_09_05.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21bd57-9d5a-4d3f-b6ca-7cee737f8e65",
   "metadata": {
    "id": "4e21bd57-9d5a-4d3f-b6ca-7cee737f8e65"
   },
   "source": [
    "### Binary Representation of Floats\n",
    "- Sign bit\n",
    "- Exponent bits\n",
    "- Mantissa bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fade62aa-bb72-4c67-b7f4-76232ba9ec59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fade62aa-bb72-4c67-b7f4-76232ba9ec59",
    "outputId": "90c90522-a361-4ecc-c446-52c8ce2b9a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Floating Point Number:3.1457898\n"
     ]
    }
   ],
   "source": [
    "num = 3.1457898\n",
    "print(f\"Sample Floating Point Number:{num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad346123-cc0a-4e64-ba52-5e231933a7f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad346123-cc0a-4e64-ba52-5e231933a7f0",
    "outputId": "eab07fc3-e487-41f9-94a6-0b8d8448ba29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 representation of 3.1457898:\n",
      "Sign: 0\n",
      "Exponent: 10000000\n",
      "Fraction: 10010010101010010011111\n"
     ]
    }
   ],
   "source": [
    "def float32_to_binary(num):\n",
    "    return ''.join(f'{b:08b}' for b in struct.pack('!f', num))\n",
    "\n",
    "binary = float32_to_binary(num)\n",
    "\n",
    "print(f\"Float32 representation of {num}:\")\n",
    "print(f\"Sign: {binary[0]}\")\n",
    "print(f\"Exponent: {binary[1:9]}\")\n",
    "print(f\"Fraction: {binary[9:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e544f8-2d9c-4498-af70-8e016c602ca8",
   "metadata": {
    "id": "96e544f8-2d9c-4498-af70-8e016c602ca8"
   },
   "source": [
    "### Different Types of Floats\n",
    "\n",
    "- FP32\n",
    "- FP16\n",
    "- bFloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d725e79a-adf2-48a0-b45e-357c06fc8059",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d725e79a-adf2-48a0-b45e-357c06fc8059",
    "outputId": "11075f32-2542-40b0-89ae-ad286e755f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32: [3.1457899]\n",
      "Float16: [3.146]\n"
     ]
    }
   ],
   "source": [
    "# Create arrays with different float types\n",
    "f32 = np.array([num], dtype=np.float32)\n",
    "f16 = np.array([num], dtype=np.float16)\n",
    "\n",
    "print(f\"Float32: {f32}\")\n",
    "print(f\"Float16: {f16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d6b8c8-f625-4b3a-8ff3-353f38f5674d",
   "metadata": {
    "id": "94d6b8c8-f625-4b3a-8ff3-353f38f5674d"
   },
   "outputs": [],
   "source": [
    "og_scalar = torch.scalar_tensor(num)\n",
    "fp16_scalar = og_scalar.to(dtype=torch.float16)\n",
    "bf16_scalar = og_scalar.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af64e0e9-d107-406e-89d1-fb0b8a24271d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af64e0e9-d107-406e-89d1-fb0b8a24271d",
    "outputId": "1b58d1a5-12a5-463b-d0a4-721bf4b9f1c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Float32: 3.145789861679077\n",
      "Torch Float16: 3.146484375\n",
      "Torch bFloat16: 3.140625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch Float32: {og_scalar}\")\n",
    "print(f\"Torch Float16: {fp16_scalar}\")\n",
    "print(f\"Torch bFloat16: {bf16_scalar}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cac62e7-e42c-4c80-bf8f-0c2d328546e9",
   "metadata": {
    "id": "7cac62e7-e42c-4c80-bf8f-0c2d328546e9"
   },
   "source": [
    "## Quantization\n",
    "Quantization aims to reduce the number of bits needed to store these weights by binning floating-point values into lower-precision buckets. This reduces memory usage with minimal impact on performance, as small precision losses are often acceptable. \n",
    "<img src=\"./assets/ch_09_04.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0f25fa8-faee-4a31-86c6-570f77224511",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0f25fa8-faee-4a31-86c6-570f77224511",
    "outputId": "d86d95b4-be5c-43ae-e764-77bf930adb0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.875, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_x = -np.ceil([num])[0]\n",
    "max_x = np.ceil([num])[0]\n",
    "scale = 255/(max_x-min_x)\n",
    "zero_point = -round(scale*min_x)-128\n",
    "scale,zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2833335-4601-4633-849e-5978177f21a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2833335-4601-4633-849e-5978177f21a1",
    "outputId": "bb729e5f-176d-4010-c532-eaf120cd9c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_quant = round(scale*og_scalar.numpy()+zero_point)\n",
    "x_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63809a33-ba42-45da-8d13-652363d141f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63809a33-ba42-45da-8d13-652363d141f0",
    "outputId": "93fe29f8-20df-4628-f97a-a4c0d97a8433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1372549019607843"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dequant = (x_quant-zero_point)/scale\n",
    "x_dequant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed12ca-d7e5-406a-b7ea-0d0e71f096de",
   "metadata": {
    "id": "01ed12ca-d7e5-406a-b7ea-0d0e71f096de"
   },
   "source": [
    "### Quantization using Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd56dd8-07cb-4ac4-a907-1326ce6beb94",
   "metadata": {
    "id": "5dd56dd8-07cb-4ac4-a907-1326ce6beb94"
   },
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ❗ <b>Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf481e0-6c3d-438f-a1f0-900163ea29b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bf481e0-6c3d-438f-a1f0-900163ea29b9",
    "outputId": "eeee32f5-3a1a-4288-aba1-49d0ff75e24e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., size=(), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=31.875, zero_point=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qscalar = torch.quantize_per_tensor(og_scalar,torch.scalar_tensor(scale),torch.scalar_tensor(zero_point),torch.qint8)\n",
    "qscalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb675447-f05f-4097-8788-919112168d80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb675447-f05f-4097-8788-919112168d80",
    "outputId": "54c03a05-aa7f-4312-c6e4-2999d9e40a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type Original Scalar:torch.float32\n",
      "Data Type Quantized Scalar:torch.qint8\n",
      "Integer Representation of Quantized Scalar:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Type Original Scalar:{og_scalar.dtype}\")\n",
    "print(f\"Data Type Quantized Scalar:{qscalar.dtype}\")\n",
    "print(f\"Integer Representation of Quantized Scalar:{qscalar.int_repr()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca5818-a3e3-4d0d-ae9b-129241f1f40d",
   "metadata": {
    "id": "f7ca5818-a3e3-4d0d-ae9b-129241f1f40d"
   },
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ❗ <b>Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a198bf-90e7-48d6-bf4e-13fac2d4b361",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25a198bf-90e7-48d6-bf4e-13fac2d4b361",
    "outputId": "2702b0f5-05ef-47cf-d2d0-28016835ffab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1458, size=(), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.012336430830114029,\n",
       "       zero_point=-128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq_scalar = torch.quantize_per_tensor_dynamic(og_scalar,torch.qint8,False)\n",
    "dq_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1274d24-c448-4d5c-9594-e6a388e655b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1274d24-c448-4d5c-9594-e6a388e655b7",
    "outputId": "f58a8070-ad70-4ce2-c7cc-141d0bda7fa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type Dynamically Quantized Scalar:torch.qint8\n",
      "Integer Representation of Dynamically Quantized Scalar:127\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Type Dynamically Quantized Scalar:{dq_scalar.dtype}\")\n",
    "print(f\"Integer Representation of Dynamically Quantized Scalar:{dq_scalar.int_repr()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab6f796-e4ea-4904-ba2b-ad30a31a76f2",
   "metadata": {
    "id": "8ab6f796-e4ea-4904-ba2b-ad30a31a76f2"
   },
   "source": [
    "## Post Training Quantization\n",
    "\n",
    "Post-training quantization (PTQ), unlike mixed precision training, is performed after the model has been fully trained in high precision. In PTQ, weights are converted to lower-precision formats such as int8 or bfloat16, with techniques like static quantization using pre-calibrated scaling factors or dynamic quantization, which adjusts on-the-fly at runtime. PTQ is particularly advantageous for deployment scenarios, where reduced memory and latency are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AP2knprcr7vZ",
   "metadata": {
    "id": "AP2knprcr7vZ"
   },
   "source": [
    "### Torch Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "yJgIlqr0sNaz",
   "metadata": {
    "id": "yJgIlqr0sNaz"
   },
   "outputs": [],
   "source": [
    "MODEL = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pxROgU80r7Aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxROgU80r7Aa",
    "outputId": "43676c85-2acc-439c-f57b-2d95fca7ddfa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31Qf8u1psPVl",
   "metadata": {
    "id": "31Qf8u1psPVl"
   },
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "gZrxTwTbwk-s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZrxTwTbwk-s",
    "outputId": "1d582c73-3fd4-4057-93cb-c316cab383ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model's size: 3504457536 bits | 438.06 MB\n"
     ]
    }
   ],
   "source": [
    "size_model = get_model_size(model)\n",
    "print(f\"Original model's size: {size_model} bits | {size_model / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "R4uUi0jHsPP2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4uUi0jHsPP2",
    "outputId": "a02f34c6-0ea2-442a-8aaa-034c078e1d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model's size: 764995392 bits | 95.62 MB\n"
     ]
    }
   ],
   "source": [
    "size_model = get_model_size(quantized_model)\n",
    "print(f\"Quantized model's size: {size_model} bits | {size_model / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bR9CNw3ir4gc",
   "metadata": {
    "id": "bR9CNw3ir4gc"
   },
   "source": [
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324bac2-026b-4ac7-8587-4a2450f15923",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ❗ <b>This Section Needs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1496910d-ddec-4789-92bd-f7b94f6eed6f",
   "metadata": {
    "id": "1496910d-ddec-4789-92bd-f7b94f6eed6f"
   },
   "outputs": [],
   "source": [
    "MODEL = \"raghavbali/aligned-gpt2-movie_reviewer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f6f52a0-028e-4a87-a105-1f40b3c36f24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f6f52a0-028e-4a87-a105-1f40b3c36f24",
    "outputId": "c358712d-324e-492c-e6b2-6b648e3dfd4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at raghavbali/aligned-gpt2-movie_reviewer were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f45abe57-83e4-48c2-8b98-4e224bf2f716",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f45abe57-83e4-48c2-8b98-4e224bf2f716",
    "outputId": "b0d269fb-eb0a-4944-9507-940102b3977d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model's size: 3982098432 bits | 497.76 MB\n"
     ]
    }
   ],
   "source": [
    "size_model = get_model_size(model)\n",
    "print(f\"Original model's size: {size_model} bits | {size_model / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "LSY2ChYrppfl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSY2ChYrppfl",
    "outputId": "e0642223-c712-4138-87e9-b23707e01d6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of the model checkpoint at raghavbali/aligned-gpt2-movie_reviewer were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of the model checkpoint at raghavbali/aligned-gpt2-movie_reviewer were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2DXaYWstp1S9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DXaYWstp1S9",
    "outputId": "b2a073ca-f64e-41e2-ba23-7f0ac01fc473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after 8bit quantization: 1311571968 bits | 163.95 MB\n",
      "Model size after 4bit quantization: 971833344 bits | 121.48 MB\n"
     ]
    }
   ],
   "source": [
    "size_model_4bit = get_model_size(model_4bit)\n",
    "size_model_8bit = get_model_size(model_8bit)\n",
    "\n",
    "print(f\"Model size after 8bit quantization: {size_model_8bit} bits | {size_model_8bit / 8e6:.2f} MB\")\n",
    "print(f\"Model size after 4bit quantization: {size_model_4bit} bits | {size_model_4bit / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2g4Ue_FzqVR8",
   "metadata": {
    "id": "2g4Ue_FzqVR8"
   },
   "source": [
    "Confirm if the models still work as intended after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "v-aFCXtmqJYg",
   "metadata": {
    "id": "v-aFCXtmqJYg"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"King Kong\", return_tensors=\"pt\", return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "UqnAGMvVqgPg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqnAGMvVqgPg",
    "outputId": "9db07f34-4884-4ff6-996c-b66f34a111db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "og_start= time()\n",
    "outputs_og = model.generate(**inputs,\n",
    "                            max_new_tokens=25,\n",
    "                            temperature=0.8,\n",
    "                            do_sample=True,\n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "og_end= time()\n",
    "q4_start= time()\n",
    "outputs_4bit = model_4bit.generate(**inputs,\n",
    "                            max_new_tokens=25,\n",
    "                            temperature=0.8,\n",
    "                            do_sample=True,\n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "q4_end= time()\n",
    "q8_start= time()\n",
    "outputs_8bit = model_8bit.generate(**inputs,\n",
    "                            max_new_tokens=25,\n",
    "                            temperature=0.8,\n",
    "                            do_sample=True,\n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "q8_end= time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "HaKOCMqlquK5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaKOCMqlquK5",
    "outputId": "d913f21f-423f-4c4f-d50f-72b23fcf8120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Model Outputs::\n",
      "***************\n",
      "\n",
      "Original Model:(1.6615946292877197)\n",
      "---------------\n",
      "King Kong and the Killing Joke is the best in modern cinema. The acting is great, the direction is wonderful, the performances are\n",
      "\n",
      "8bit Model:(1.7423856258392334)\n",
      "---------------\n",
      "King Kong: Skull Island - Full HD Remaster - 2.5/10.\n",
      "\n",
      " video is beautiful and the music is great\n",
      "\n",
      "4bit Model:(4.4493348598480225)\n",
      "---------------\n",
      "King Kong movie, then I'd like to see a big action movie with an action movie attached. The first two thirds of the movie\n"
     ]
    }
   ],
   "source": [
    "print(\"::Model Outputs::\")\n",
    "print(\"*\"*15)\n",
    "print()\n",
    "print(f\"Original Model:({og_end-og_start})\")\n",
    "print(\"-\"*15)\n",
    "print(tokenizer.decode(outputs_og[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(f\"8bit Model:({q8_end-q8_start})\")\n",
    "print(\"-\"*15)\n",
    "print(tokenizer.decode(outputs_8bit[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(f\"4bit Model:({q4_end-q4_start})\")\n",
    "print(\"-\"*15)\n",
    "print(tokenizer.decode(outputs_4bit[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hZePqfezvPlU",
   "metadata": {
    "id": "hZePqfezvPlU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
