{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpxmDr7OPwTM"
      },
      "outputs": [],
      "source": [
        "pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQxhYiFfHTEt"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tllsGFZt5aL"
      },
      "outputs": [],
      "source": [
        "! curl https://people.eecs.berkeley.edu/~hendrycks/data.tar -o data.tar\n",
        "! tar -xvf data.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fBQ2DLKuBsS"
      },
      "outputs": [],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXGxX3HPumcJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/dev/high_school_geography_dev.csv', header = None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpt9rLA2u8nj"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeYJ-mAAvMfP"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\".join(df.iloc[0,:-1])+\"\\nAnswer:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU9dPzzWUjUr"
      },
      "outputs": [],
      "source": [
        "pipeline(\"The following are multiple choice questions (with answers) about high school geography, provide the answer from the four listed options using A, B, C, D\"+\"\\n\".join(df.iloc[0,:-1])+\"\\nAnswer:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q4M3KfBy5y37"
      },
      "outputs": [],
      "source": [
        "! curl https://raw.githubusercontent.com/rowanz/hellaswag/refs/heads/master/data/hellaswag_train.jsonl -0 hellaswag_train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffg2Jy3X1lPi"
      },
      "outputs": [],
      "source": [
        "pip install human-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIYMKcMcZQo4"
      },
      "outputs": [],
      "source": [
        "from human_eval.data import write_jsonl, read_problems\n",
        "\n",
        "problems = read_problems()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efpFSvAVCK57"
      },
      "outputs": [],
      "source": [
        "len(problems)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RviXbmACJI-"
      },
      "outputs": [],
      "source": [
        "answer = pipeline(problems['HumanEval/0'][\"prompt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3_oZw7DbGoi"
      },
      "outputs": [],
      "source": [
        "pipeline.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFjrg82Ea6H6"
      },
      "outputs": [],
      "source": [
        "print(list(problems['HumanEval/0'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECG2-0taDGaS"
      },
      "outputs": [],
      "source": [
        "print(problems['HumanEval/0']['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kJQy2kiCb6t"
      },
      "outputs": [],
      "source": [
        "print(problems['HumanEval/0']['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQmPeFhUashy"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(answer[0][\"entry_point\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep0cQdk1a5XP"
      },
      "outputs": [],
      "source": [
        "answer = pipeline(problems[\"HumanEval/0\"][\"prompt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GUBHGOjlX3I"
      },
      "outputs": [],
      "source": [
        "print(answer[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp8WECAaoAiE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "exec(answer[0][\"generated_text\"])\n",
        "exec(problems['HumanEval/0']['test'])\n",
        "check(eval(problems['HumanEval/0'][\"entry_point\"]))\n",
        "\n",
        "def wronga(numbers, threshold):\n",
        "  pass\n",
        "\n",
        "check(wronga)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! curl https://raw.githubusercontent.com/rowanz/hellaswag/refs/heads/master/data/hellaswag_train.jsonl -o hellaswag_train.jsonl"
      ],
      "metadata": {
        "id": "bV6HXUeFfwAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "hswag = pd.read_json(path_or_buf='hellaswag_train.jsonl', lines=True)\n",
        "\n",
        "hswag.head()"
      ],
      "metadata": {
        "id": "Li7dKZ9af066"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline(\"Pick the best ending to the quoted context from the four listed options using label 0,1,2,3: \"+\"the context is: \\\"\"+hswag.loc[0,'ctx']+\"\\\"\" \"\\n. The endings are: \"+\"\\n\".join(hswag.loc[0,\"endings\"])+\n",
        "\n",
        "\"\\n. The best ending for this context and the reasoning is: \")"
      ],
      "metadata": {
        "id": "ntpV78Etf_A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdB8olfhskJ7"
      },
      "outputs": [],
      "source": [
        "pip install -U flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQbWGSPGy2i6"
      },
      "outputs": [],
      "source": [
        "pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi-QmfW7yZU7"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from accelerate.utils import BnbQuantizationConfig\n",
        "quantization_config = BnbQuantizationConfig(\n",
        "    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "model = \"mistralai/Mixtral-8x7B-v0.1\"\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    model_kwargs = {\"torch_dtype\":torch.bfloat16,\n",
        "    \"attn_implementation\":\"flash_attention_2\",\n",
        "    \"max_length\":1000},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEG41FfgtpG8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAnVcKD4CDtI"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "model = \"databricks/dolly-v2-12b\"\n",
        "\n",
        "\n",
        "\n",
        "pipeline = pipeline(\n",
        "\n",
        "\"text-generation\",\n",
        "\n",
        "model=model,\n",
        "\n",
        "torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eaxl9l4tdkZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "torch.set_default_dtype(torch.bfloat16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hpcai-tech/grok-1\", trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"hpcai-tech/grok-1\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "text = \"Replace this with your text\"\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.cuda()\n",
        "attention_mask = torch.ones_like(input_ids)\n",
        "generate_kwargs = {}  # Add any additional args if you want\n",
        "inputs = {\n",
        "    \"input_ids\": input_ids,\n",
        "    \"attention_mask\": attention_mask,\n",
        "    **generate_kwargs,\n",
        "}\n",
        "outputs = model.generate(**inputs)\n",
        "print(outputs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}