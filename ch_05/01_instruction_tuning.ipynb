{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b9f9be-bff1-47dc-8be0-576ef6a557ff",
   "metadata": {
    "id": "19b9f9be-bff1-47dc-8be0-576ef6a557ff"
   },
   "source": [
    "# Instruction Tuning\n",
    "\n",
    "Instruction tuning is form of fine-tuning that enhances a model's ability to generalize across diverse tasks. This concept is particularly useful in making models more adaptable and efficient in understanding and executing new instructions, even those they haven't been explicitly trained on.\n",
    "\n",
    "## Ok, But What is Instruction Tuning?\n",
    "Instruction tuning differs from supervised fine-tuning (SFT) approach primarily in the nature of the training data. While both methods involve training on **input-output pairs**, instruction tuning adds a critical layer: **instructions**. This additional context helps the model understand the task it is being asked to perform, leading to improved generalization to unseen tasks. Also, as we will see in this notebook, one of the ways of doing instruction tuning helps us skip the trouble of designing task specific heads or loss functions!\n",
    "\n",
    "## Key Differences:\n",
    "- **Supervised Fine-Tuning**: Trains models using input examples and their corresponding outputs.\n",
    "- **Instruction Tuning**: Augments the input-output pairs with instructions, enhancing the model's ability to generalize to new tasks.\n",
    "\n",
    "\n",
    "## Examples\n",
    "**Supervised Fine-Tuning:**\n",
    "\n",
    "- **Input**: \"Translate this sentence to French: 'The cat is on the mat.'\"\n",
    "- **Output**: \"Le chat est sur le tapis.\"\n",
    "\n",
    "**Instruction Tuning:**\n",
    "\n",
    "- **Instruction:** \"Translate the following sentence to French.\"\n",
    "- **Input:** \"The cat is on the mat.\"\n",
    "- **Output:** \"Le chat est sur le tapis.\"\n",
    "\n",
    "By incorporating instructions, the model gains a better understanding of the task, leading to more robust performance across a wider range of tasks.\n",
    "\n",
    "In this notebook, we will go deeper into the mechanics of instruction tuning and tune our own model.\\\n",
    "\n",
    "> Even though the original work by. Ouyang et. al. presents a model based off GPT-3, for the sake of learning we will leverage GPT-2. If you have larger compute/more GPU-RAM available, feel free to experiment with larger models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659a9a1-2989-4382-8173-a59434d253e2",
   "metadata": {},
   "source": [
    "## Instruction Tuned GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4f3d782-174b-4c55-8d11-8b9a52c34fe9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4f3d782-174b-4c55-8d11-8b9a52c34fe9",
    "outputId": "43f61585-b11d-4ef4-f76b-8df0c0ae1fee"
   },
   "outputs": [],
   "source": [
    "# !pip3 install scikit-learn==1.5.1\n",
    "# !pip3 install transformers==4.43.4\n",
    "# !pip3 install datasets==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a457e201-f112-4bbe-9ebb-04a39961ce73",
   "metadata": {
    "id": "a457e201-f112-4bbe-9ebb-04a39961ce73"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xvqHlVgEzhXO",
   "metadata": {
    "id": "xvqHlVgEzhXO"
   },
   "source": [
    "### Select Compute Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d61175-df48-426c-8180-b4f445a88587",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7d61175-df48-426c-8180-b4f445a88587",
    "outputId": "af72c95b-e174-47a1-d57a-2d8844211dd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend Accelerator Device=mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "# MPS/Apple Silicon does not work as intended for this pipeline\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = 0\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    Tensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    DEVICE_ID = -1\n",
    "print(f\"Backend Accelerator Device={DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Hd8MQufzZRV",
   "metadata": {
    "id": "_Hd8MQufzZRV"
   },
   "source": [
    "### Connect to Hugginface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e401adf-db30-4675-a11b-bce1aa031457",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "4c7a73c549e54764a75cdf9e808241b2",
      "0961d0d391e744688197a4176e7e5934",
      "a0f522be853e4542ad1981d8710d038b",
      "1088960a976a40e2b37a9038dbacea64",
      "5a344a1114184275bf1a56c869e5c986",
      "8de2330c6e7949a89ee090209f9aaf94",
      "aee1aeffe46c4f2da11289cd1f7fea72",
      "a2e53e6faf914610b687c7e6cf8dc22b",
      "781f100153d9416587b6d45df157eb7f",
      "f7b5433196ce4921a951b39b4155e77c",
      "90bc6abe7bfd4b95a1adfb47a575cc53",
      "687404769c4f430d8351249d57df5745",
      "89a0f2d00aed4514bf36001abc18b24e",
      "0c9130a455b5434fb880243cc73ff7b9",
      "a26c610033bc4758a3c92616cd54653d",
      "093923b66b0f4ea18b567759d5957278",
      "1cf5b3e7b41c49c0abf28346b3f8bd8c",
      "b59c758621b54245a8da969132f34bb9",
      "9e3cad495aa04cb4ad11922abe44f591",
      "36558cd4c04e4fcca04b3c37500e24cd",
      "b307444cd1e74d0b8eda46b2d833c67c",
      "91f99314e67b46b9a81dcd9b3b2a1c15",
      "2ef5157bd9c846f7b6d96b0629b0efa7",
      "c67fe427c6af4f06a26fa4068aee0321",
      "ada3ebec69154588b83aaa3dc35b068b",
      "5feb5ff13f424dc484efa8c3b317c789",
      "780fd49e5aff429984a1a98a59538dee",
      "902192d63c8f40f99cb870d18c361fdb",
      "df0b081561fe42679888c386edaea6d9",
      "d251df69f360437ba8fe401950716ba6",
      "65fa206c70224ac185b93081bc50635e",
      "53f9f25674bf48cb81bb1e1edd31a78c"
     ]
    },
    "id": "0e401adf-db30-4675-a11b-bce1aa031457",
    "outputId": "e6f852d8-df53-410f-ab61-d7683ff44452"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5cfc99534e43cab0173d38fbd7e70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89ef57-48d5-4a90-b428-179210a3126c",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dGhfk3v0F9W",
   "metadata": {
    "id": "4dGhfk3v0F9W"
   },
   "outputs": [],
   "source": [
    "TOKENIZER = \"gpt2\"\n",
    "MODEL = \"raghavbali/gpt2-finetuned-headliner\"\n",
    "OUTPUT_MODEL_NAME = \"raghavbali/gpt2-instruct-tuned-translator2\"\n",
    "DATASET = 'news_english_german_instruction_dataset_20240909.json'\n",
    "PUSH_TO_HUB = False if OUTPUT_MODEL_NAME.split('/')[0]=='raghavbali' else True # do not push to hub if you are simply trying out generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e4891-50a4-42a0-955b-06c6cf62e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"do_sample\":True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"eos_token_id\":50256,\n",
    "    \"max_new_tokens\": 50,\n",
    "}\n",
    "generate_config = GenerationConfig(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3RGl8I50zH_E",
   "metadata": {
    "id": "3RGl8I50zH_E"
   },
   "source": [
    "### Get Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e15f55-5921-4489-9a9d-2544626b6acf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340,
     "referenced_widgets": [
      "679c68521b064757b0b13c7c99156344",
      "23a72a6d055e4b77b03460306d849436",
      "436a0160fe714b5f821a29d89bf52d0e",
      "26445b1867c34d689b7accb5ce44ca75",
      "4c693aebfbf74ff680cc2c2c810f17f7",
      "3140ea1cc9c449d9a8b910f15d811891",
      "aff20c1d73e34e329a112aecfcbdc873",
      "aab9011ac99b4971850a1385b2b76eb1",
      "0636d43069244cd88897e2da060dda77",
      "dc2775eacb0e426f8978531a28c9ade3",
      "6c590e0e3e1548b2a3e0e81009a0f9b1",
      "c24b1128c7a04fc8b04d3bf3aa66b5f3",
      "284304e12c284948b9e67f16be21385d",
      "dd4eb65fe25c436d872e58023bc5522f",
      "53e16cc53af54a18a5c65be7730c7c91",
      "3fc48695e40749e4a31d6d613f3ced10",
      "2d2910c31b21424bb0d4d8bcda6c703c",
      "325e6773a4f544b193383d187afda7e0",
      "8102715703d44dc2b8eea742b381b2df",
      "97deeb112e3b4adeb6503064b093f809",
      "b71eb6511ced4d6bbc62ebb1f38a397d",
      "f12fe48134924b90ba9ac6ad0414e56f",
      "0f756ac7d41e443b93f9ab6f6ca8972c",
      "f178879192224548bbafa1794d19021f",
      "59e668ebc8414fd0b0e4cebb2b344cc4",
      "a004f7ac4f7b4b3fb48465c4e5b4a32c",
      "7bb45727ce324e36b0c2b7b7ca57eece",
      "26a3d9507bad40c98e991e0aea16b1e2",
      "b7294b41fbb047628f61bf6c80e7195a",
      "6d12cd3d550c4be8819d4e167259f04c",
      "76e40f0916c747f9ac8aa6d1e8197cfa",
      "5c9334735e614b40b6f17f4b92b9e08f",
      "c2dda89fee6f4ad392202f093a41554b",
      "e99986a00ddb4c2bbb954f18ab8a8d00",
      "a72d6d9acaba46a1af45c284713bc77c",
      "f1242d8b601541dba47902bc272c4aa2",
      "5761ccbfe67049fb9980dccb484c7150",
      "8417d00cadb1465a8e278d88f821dca9",
      "2ba218b66582463d8080aa5fd2d0af4f",
      "9c1889cb32d643c9ad8c954fd24c7629",
      "b03e3ab7a9f34c01823e2a1678efa91d",
      "f1501a7e0cde40a98a7e00df63cbb42f",
      "e3fb1f921ddc467fac6af6ba24cf3a40",
      "babb5d4582f54c239de9dbed8d0a48d4",
      "eac82f1db01442239c6af794be243051",
      "cda3c62c6c2c4bebae4a0ad2f3a70429",
      "f0d312138a474e70a6fef3cf6b42c8d9",
      "ae40a2b417134171aa4417bc40e24a69",
      "d7c22a59b5bd4c899a700250b3347065",
      "f262e8a4fb734ef1a7705b3b8c4a6e4d",
      "4048a09c2a1345aa867e28a058b1c575",
      "8f2eadce93854568bc826afc0d4a99d3",
      "7bdb0e6552a646a78a30c0f63d13bb94",
      "39f52145de2d4dbc9aabd134107a3cb2",
      "e144b1c4df344fc185c2f2a881f66910"
     ]
    },
    "id": "75e15f55-5921-4489-9a9d-2544626b6acf",
    "outputId": "06fbb86c-b4c0-45f0-a930-512ec80b71eb"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER,clean_up_tokenization_spaces=True)#,pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffef4bc-e884-4509-9636-eedb039e8961",
   "metadata": {
    "id": "fffef4bc-e884-4509-9636-eedb039e8961"
   },
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ec0206-0b2c-4569-872e-a58af385686d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15ec0206-0b2c-4569-872e-a58af385686d",
    "outputId": "8abd79eb-ae10-409a-cdc4-f87a46c3cbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records=6220\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "instruction_dataset = list()\n",
    "with open(DATASET, \"r\") as jsonfile:\n",
    "    instruction_dataset = json.load(jsonfile)\n",
    "print(f\"Total Records={len(instruction_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cLVac3FmTNxO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLVac3FmTNxO",
    "outputId": "44a89e8f-fff3-407b-baa8-e0b12a2b0c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records Remaining=6037\n"
     ]
    }
   ],
   "source": [
    "# basic cleanup to remove very short or blank translations\n",
    "instruction_dataset = [{\n",
    "    'input':record['input'],\n",
    "    'output_gpt4omini':record['output_gpt4omini']\n",
    "} for record in instruction_dataset if record['output_gpt4omini']!='#' and len(record['output_gpt4omini'])>2]\n",
    "print(f\"Total Records Remaining={len(instruction_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05188d98-d126-46f7-aa2b-f9d33483575d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05188d98-d126-46f7-aa2b-f9d33483575d",
    "outputId": "b5584fa2-281c-4b90-a358-f2a9f3876fda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test= train_test_split(instruction_dataset[:5000],test_size=0.1, random_state=42)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gV6cE0gZzNAn",
   "metadata": {
    "id": "gV6cE0gZzNAn"
   },
   "outputs": [],
   "source": [
    "# tokenization function\n",
    "def tokenize_function(examples):\n",
    "    examples[\"text\"] = [f\"###Translate to German:{ed['input']}\\n###Output:{ed['output_gpt4omini']}<|endoftext|>\" for ed in examples[\"text\"]]\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d2dad66-49cd-4af9-9133-9d2add0d1bd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172,
     "referenced_widgets": [
      "edba4bed48104c4e8c4aec81e2d8e074",
      "872f642be9ff4734bbc04a58f8f9684f",
      "3e8e9ac5d516456eac1f2f01097e22b9",
      "3e25d867c2a246fb8d3fddf590db4ed9",
      "88bd4df4fb74407b9670411eedd3fe54",
      "45efd2ec71b24ab08306a1cc2ba3a30c",
      "93ff01456c044254a98e4420903cd3cd",
      "053adab2e17748fc91e475666f90d9a1",
      "13c15a9f273f40cbb1b4d0d822fab3be",
      "813c232efe85458796af393e4527d8fb",
      "824c2b209d1d4451a9cd449a03cbc61e",
      "15188e590d9d40fbbebcbaa279724ea0",
      "6428cc25e3b14d2cb010933b439bf775",
      "1cb3c605896d491a82b79c2a0644bcad",
      "28e200b8b5b8457ca58de4bee705158d",
      "6337bd7a531342d68d2accec5d57f2ed",
      "0a3dbb9648504137a12790958573c412",
      "e1d10e76d5ad43e29c487b0a6b04a362",
      "c5f06f3aae5f4f3390aa94c17194561c",
      "9bcc4a7bff5a41a9811ec33bb1d746c8",
      "e9a5419abd4f41289496c5719a3c46a6",
      "5e78a719dd8d46b0b37556dc8a8d71e8"
     ]
    },
    "id": "8d2dad66-49cd-4af9-9133-9d2add0d1bd0",
    "outputId": "4f929daf-2021-48d8-ee05-959de81efc1c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686de6c101064f6c952a66d41dc28d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0638473a56a4651b80899fb9800850f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized datasets\n",
    "tokenized_train_dataset = Dataset.from_dict({'text':X_train}).map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = Dataset.from_dict({'text':X_test}).map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42589349-ece5-4b79-9159-a81112e500df",
   "metadata": {
    "id": "42589349-ece5-4b79-9159-a81112e500df"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445d7990-3dff-417e-bee9-6c45baed0d8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "445d7990-3dff-417e-bee9-6c45baed0d8d",
    "outputId": "347d081a-fd28-4655-9552-44ebe5cf8739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###Translate to German:explosion rips through mexican fireworks market\\n###Output:Eine Explosion ersch\u00fcttert einen mexikanischen Feuerwerksmarkt.<|endoftext|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample output\n",
    "tokenizer.decode(tokenized_test_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035383de-b4fb-470e-8448-d53c62c60911",
   "metadata": {
    "id": "035383de-b4fb-470e-8448-d53c62c60911"
   },
   "source": [
    "## Prepare Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d75f6b17-f49a-4284-822c-19903227120c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "565a1781f14942448ca3cd7c7b37fe89",
      "aeabff09bc6a4f7d80e5f0821e13f0fe",
      "53df07c1daee4934884a6644c5f9ffda",
      "a202dce3c80a4886b5c2306da16c2e94",
      "94e483fa5c5d414c943480f8764d12d4",
      "37254180780149afac84aecd9936f120",
      "a13b3f23fcfa415ca7fafec053e1fdb2",
      "6d03239c7daf481c88e9b116dc234b72",
      "13518ad766de4d748db13ec1639ea786",
      "5e4455a7bf6b44c49ae29e8daba97cd2",
      "35d69abcebf24b28b6b44b38151bc18c",
      "9f95448e2b0249e8b73e3a4aaa28f4ec",
      "0d4d3585f23145a19de5957279ef6000",
      "1db375988d3141c4b120991eaa045018",
      "129d17c3826c44c4b3427d978c889914",
      "26978d834ebf42ec99709ab828534974",
      "12a8a9d887f44331b1f02772d3a0e0f0",
      "f20ac4ef8d5d48878efc97583956c5e8",
      "2b50d8105e0c4c9fae94711c10e1fbeb",
      "95c37eedc82a449985015f61f9e6c56e",
      "7a306f9fdd8d4a2b8312faf79c85d5f1",
      "591dad1ab4f445f8992c42c7422f72cb",
      "6b6798978422419d988e90db721ebb1b",
      "7c1126cc2af547c8b030cec4e53e6388",
      "a6dbfca96dfe411cb56fec0ec5111fa5",
      "9f6d0f68e8f042388c09ea128225276e",
      "d506c1cef83e41b2a318d37c76675017",
      "643db4ffad6746d59f8fe2a8d6063d47",
      "74b07ab9051246a585deef02467ad4e0",
      "dca24796d8754ca39478b2a5a01bd3f6",
      "12371b343a544e46ba8fac20eae36a8e",
      "c172de396ed24cff8afe893498b98ec4",
      "b162f85681e5470bb2d1340cbeca7e83"
     ]
    },
    "id": "d75f6b17-f49a-4284-822c-19903227120c",
    "outputId": "163f1afd-8c61-4782-a1af-086dee51044b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565a1781f14942448ca3cd7c7b37fe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/984 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f95448e2b0249e8b73e3a4aaa28f4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6798978422419d988e90db721ebb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL,device_map=\"auto\",).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ff3d217-f33f-41f5-93fb-b462b88036d3",
   "metadata": {
    "id": "1ff3d217-f33f-41f5-93fb-b462b88036d3"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    OUTPUT_MODEL_NAME, #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    eval_steps = 16, # Number of update steps between two evaluations.\n",
    "    save_steps=32, # after # steps model is saved\n",
    "    warmup_steps=4,# number of warmup steps for learning rate scheduler\n",
    "    push_to_hub=PUSH_TO_HUB,\n",
    "    logging_steps=16,\n",
    "    #use_mps_device=True, # uncomment this if you have MPS available\n",
    "    #use_cpu=True # comment this if you have GPU available\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sVw0HRFVg8Pn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVw0HRFVg8Pn",
    "outputId": "dac320bc-af6f-4333-d5b8-abfa1d5a7499"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "148ff44c-f82e-41de-a595-23aafef9ea93",
   "metadata": {
    "id": "148ff44c-f82e-41de-a595-23aafef9ea93"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b22d1aa-0820-4096-8a3d-d80244d4a619",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6b22d1aa-0820-4096-8a3d-d80244d4a619",
    "outputId": "a9a7144a-c04a-4036-c55f-7e6dab210c50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 21:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>3.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>3.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.943200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>2.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>2.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>2.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>2.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>2.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>2.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>2.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>2.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>2.534200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>2.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>2.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>2.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>2.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>2.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>2.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.459300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=564, training_loss=3.401080123076202, metrics={'train_runtime': 1268.3087, 'train_samples_per_second': 7.096, 'train_steps_per_second': 0.445, 'total_flos': 919486242324480.0, 'train_loss': 3.401080123076202, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823eb84-895c-4745-bac0-054772cedab7",
   "metadata": {
    "id": "c823eb84-895c-4745-bac0-054772cedab7"
   },
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c382996-2764-45c4-bc80-0148a9162e02",
   "metadata": {
    "id": "0c382996-2764-45c4-bc80-0148a9162e02"
   },
   "source": [
    "## Let us Instruct Some Translations!\n",
    "\n",
    "> **Note** : The model seems to have picked up german vocabulary but the translations do not make sense all the time.\n",
    "\n",
    "> Contrast this (as an exercise) with the performance of the pretrained version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c1426-93d1-4a7d-9727-8833cf9378c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you have not setup the training in this session\n",
    "# tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d73670c5-a1f6-40e6-998c-56165b26654f",
   "metadata": {
    "id": "d73670c5-a1f6-40e6-998c-56165b26654f"
   },
   "outputs": [],
   "source": [
    "# load the instruction-tuned model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(MODEL,device_map=\"auto\",).to(DEVICE)\n",
    "inst_tuned_model = AutoModelForCausalLM.from_pretrained(OUTPUT_MODEL_NAME).to(DEVICE)\n",
    "#-> comment .to(DEVICE) if you are using Apple Silicon\n",
    "\n",
    "pretrained_model.resize_token_embeddings(len(tokenizer))\n",
    "inst_tuned_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# setup the generation pipeline\n",
    "translator_pipeline = pipeline('text-generation',\n",
    "                     model=inst_tuned_model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     pad_token_id=0,\n",
    "                     eos_token_id=50256,\n",
    "                     # device=DEVICE,\n",
    "                     model_kwargs=generate_kwargs\n",
    "                    )\n",
    "\n",
    "pretrained_pipeline = pipeline('text-generation',\n",
    "                     model=pretrained_model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     pad_token_id=0,\n",
    "                     eos_token_id=50256,\n",
    "                     # device=DEVICE,\n",
    "                     model_kwargs=generate_kwargs\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2f94852-1ffa-4c03-a9a7-41435c6b02d8",
   "metadata": {
    "id": "a2f94852-1ffa-4c03-a9a7-41435c6b02d8"
   },
   "outputs": [],
   "source": [
    "def get_translated_headline(_pipeline, seed_text=\"News\"):\n",
    "  return _pipeline(seed_text)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "uH1zeGaidYN4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uH1zeGaidYN4",
    "outputId": "066cb7d9-270c-4f4f-f3ef-a8c75b19a7de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction Tuned Model Response::\n",
      "###Translate to German:today is a beautiful day\n",
      "###Output:die \u00c4lle f\u00fcr drei Fahren der Erde\n",
      "\n",
      "Pretrained Model Response::\n",
      "###Translate to German:today is a beautiful day\n",
      "###Output:\n",
      "*************************\n",
      "Instruction Tuned Model Response::\n",
      "###Translate to German:john is set to meet mark\n",
      "###Output:John sich von dem Mark erkl\u00e4ren, sagt Mark\n",
      "\n",
      "Pretrained Model Response::\n",
      "###Translate to German:john is set to meet mark\n",
      "###Output:\n",
      "*************************\n",
      "Instruction Tuned Model Response::\n",
      "###Translate to German:Australia wins Gold Medal at olympics\n",
      "###Output:Australien eine Gold Medal zu den Olympierten.\n",
      "\n",
      "Pretrained Model Response::\n",
      "###Translate to German:Australia wins Gold Medal at olympics\n",
      "###Output:\n",
      "*************************\n",
      "Instruction Tuned Model Response::\n",
      "###Translate to German:Australian secures coal policy in China.\n",
      "###Output:Australiens warten der Coalen policy in China.\n",
      "\n",
      "Pretrained Model Response::\n",
      "###Translate to German:Australian secures coal policy in China.\n",
      "###Output:\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# sample strings to test\n",
    "samples= [\n",
    "    \"today is a beautiful day\",\n",
    "    \"john is set to meet mark\",\n",
    "    \"Australia wins Gold Medal at olympics\",\n",
    "    \"Australian secures coal policy in China.\"\n",
    "]\n",
    "for _str in samples:\n",
    "  input_str = f\"###Translate to German:{_str}\\n###Output:\"\n",
    "  inst_response = get_translated_headline(translator_pipeline, seed_text=input_str)\n",
    "  pt_response = get_translated_headline(pretrained_pipeline, seed_text=input_str)\n",
    "  print(\"Instruction Tuned Model Response::\")\n",
    "  print(inst_response)\n",
    "  print()\n",
    "  print(\"Pretrained Model Response::\")\n",
    "  print(pt_response)\n",
    "  print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5912e2af-6576-49fd-9ad9-63b43da657cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5912e2af-6576-49fd-9ad9-63b43da657cf",
    "outputId": "49867057-1ef0-46f7-f9aa-26243030625a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Translate to German:warner smith return for blues\n",
      "###Output:Warner Smith schnell vor den blues\n",
      "GPT-Translation:Warner Smith R\u00fcckkehr f\u00fcr Blues\n",
      "\n",
      "###Translate to German:gold coast could have superyacht marina boyle\n",
      "###Output:Gold Coast gewinnt wirtsicher schafft Marle in der Stadt Gold.\n",
      "GPT-Translation:Die Goldk\u00fcste k\u00f6nnte einen Superyacht-Hafen in Boyle haben.\n",
      "\n",
      "###Translate to German:bid offered for hamilton is\n",
      "###Output:Schlie\u00dfer, der in Brandwurf auf hamilton.\n",
      "GPT-Translation:Das Gebot f\u00fcr Hamilton ist\n",
      "\n",
      "###Translate to German:bhp ordered to assess seismic risks\n",
      "###Output:Berichkeit erfasst vor Geowarsenheit\n",
      "GPT-Translation:BHP beauftragt, seismische Risiken zu bewerten\n",
      "\n",
      "###Translate to German:nsw premier says health authorities need to watch\n",
      "###Output:Die Premierminister f\u00fcr die Entwicklung vor Gericht auf die \u00dcberlokalien\n",
      "GPT-Translation:Der Premier von New South Wales sagt, die Gesundheitsbeh\u00f6rden m\u00fcssen aufpassen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# samples from test set\n",
    "for _str in X_test[25:30]:\n",
    "  input_str = f\"###Translate to German:{_str['input']}\\n###Output:\"\n",
    "  response = get_translated_headline(translator_pipeline, seed_text=input_str)\n",
    "  print(response)\n",
    "  print(f\"GPT-Translation:{_str['output_gpt4omini']}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04a781-9b97-429c-a7ef-54f9f8e92721",
   "metadata": {
    "id": "mWZc1r2W9Q-t"
   },
   "source": [
    "## Extended Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988bcd0f-7164-4989-995d-9f4c3fb9a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"do_sample\":True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"eos_token_id\":50256,\n",
    "    \"max_new_tokens\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d1958f-578b-462b-8c90-5298ac12b5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# load the instruction-tuned model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(MODEL).to('cpu')\n",
    "inst_tuned_model = AutoModelForCausalLM.from_pretrained(OUTPUT_MODEL_NAME).to('cpu')\n",
    "#-> comment .to(DEVICE) if you are using Apple Silicon\n",
    "\n",
    "# force move to CPU for apple silicon\n",
    "# pretrained_model.to('cpu');\n",
    "# inst_tuned_model.to('cpu');\n",
    "\n",
    "pretrained_model.resize_token_embeddings(len(tokenizer))\n",
    "inst_tuned_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# setup the generation pipeline\n",
    "translator_pipeline = pipeline('text-generation',\n",
    "                     model=inst_tuned_model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     pad_token_id=0,\n",
    "                     eos_token_id=50256,\n",
    "                     #device=DEVICE,\n",
    "                     model_kwargs=generate_kwargs\n",
    "                    )\n",
    "\n",
    "pretrained_pipeline = pipeline('text-generation',\n",
    "                     model=pretrained_model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     pad_token_id=0,\n",
    "                     eos_token_id=50256,\n",
    "                     #device=DEVICE,\n",
    "                     model_kwargs=generate_kwargs\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1fcee32-9949-4272-ab88-c59331d4b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_input_str = \"News Zealand man charged with \"\n",
    "pt_input_str2 = \"australia wins gold at olympics at\"\n",
    "inst_input_str = f\"###Translate to German:farmers grow monstor tomatoes\\n###Output:\"\n",
    "inst_input_str2 = f\"###Translate to German:nsw declare heat warning\\n###Output:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc06d207-f38d-4fac-8eb5-5569b8f0af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Prompt= News Zealand man charged with ...\n",
      "--------------------------------------------------\n",
      "Instruction Tuned Model::\n",
      "News Zealand man charged with ersatz murder of 12yo boy\n",
      "###\n",
      "\n",
      "Pretrained Model::\n",
      "News Zealand man charged with urdock assault\n",
      "\n",
      "CHRISTOPHER WESSLEY\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt= australia wins gold at olympics at...\n",
      "--------------------------------------------------\n",
      "Instruction Tuned Model::\n",
      "australia wins gold at olympics at snl\n",
      "\n",
      "Pretrained Model::\n",
      "australia wins gold at olympics at nur\n",
      "\n",
      "french team beats australia on aces\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt= ###Translate to German:farmers grow monstor tomatoes\n",
      "###Output:...\n",
      "--------------------------------------------------\n",
      "Instruction Tuned Model::\n",
      "###Translate to German:farmers grow monstor tomatoes\n",
      "###Output:Schwierigkeitsprodukten Monstor-Tomat\n",
      "\n",
      "Pretrained Model::\n",
      "###Translate to German:farmers grow monstor tomatoes\n",
      "###Output: more\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt= ###Translate to German:nsw declare heat warning\n",
      "###Output:...\n",
      "--------------------------------------------------\n",
      "Instruction Tuned Model::\n",
      "###Translate to German:nsw declare heat warning\n",
      "###Output:NSW wollen danken Auszeichnungen werden\n",
      "\n",
      "Pretrained Model::\n",
      "###Translate to German:nsw declare heat warning\n",
      "###Output:nsw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in [pt_input_str,pt_input_str2,inst_input_str,inst_input_str2]:\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Prompt= {s}...\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"Instruction Tuned Model::\")\n",
    "    print(translator_pipeline(s)[0]['generated_text'])\n",
    "    print()\n",
    "    print(\"Pretrained Model::\")\n",
    "    print(pretrained_pipeline(s)[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d75a07-731a-45c6-a0b3-0316bc000877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
